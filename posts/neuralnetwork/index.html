<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    <title>GoFrendi&#39;s Articles  | Neural Network From Scratch</title>
    <meta name="HandheldFriendly" content="True">
    <meta name="MobileOptimized" content="320">

    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="generator" content="Hugo 0.55.6" />
    
    
      <META NAME="ROBOTS" CONTENT="NOINDEX, NOFOLLOW">
    

    
    
      <link href="/dist/css/app.d98f2eb6bcd1eaedb7edf166bd16af26.css" rel="stylesheet">
    

    

    
      
    

    

    <meta property="og:title" content="Neural Network From Scratch" />
<meta property="og:description" content="Implementation with Tensorflow TensorFlow is an open-source software library for dataflow programming across a range of tasks. It is a symbolic math library, and is also used for machine learning applications such as neural networks.
Unlike sklearn.neural_network, tensorflow give us more freedom to set up our neural-network.
The following implementation was taken directly from https://www.tensorflow.org/tutorials/keras/basic_classification
First of all, let&rsquo;s try to import tensorflow and keras (which is now also part of tensorflow)" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://gofrendiasgard.github.io/posts/neuralnetwork/" />
<meta property="article:published_time" content="2019-07-31T07:48:41&#43;07:00"/>
<meta property="article:modified_time" content="2019-07-31T07:48:41&#43;07:00"/>

<meta itemprop="name" content="Neural Network From Scratch">
<meta itemprop="description" content="Implementation with Tensorflow TensorFlow is an open-source software library for dataflow programming across a range of tasks. It is a symbolic math library, and is also used for machine learning applications such as neural networks.
Unlike sklearn.neural_network, tensorflow give us more freedom to set up our neural-network.
The following implementation was taken directly from https://www.tensorflow.org/tutorials/keras/basic_classification
First of all, let&rsquo;s try to import tensorflow and keras (which is now also part of tensorflow)">


<meta itemprop="datePublished" content="2019-07-31T07:48:41&#43;07:00" />
<meta itemprop="dateModified" content="2019-07-31T07:48:41&#43;07:00" />
<meta itemprop="wordCount" content="2143">



<meta itemprop="keywords" content="Macine Learning," />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Neural Network From Scratch"/>
<meta name="twitter:description" content="Implementation with Tensorflow TensorFlow is an open-source software library for dataflow programming across a range of tasks. It is a symbolic math library, and is also used for machine learning applications such as neural networks.
Unlike sklearn.neural_network, tensorflow give us more freedom to set up our neural-network.
The following implementation was taken directly from https://www.tensorflow.org/tutorials/keras/basic_classification
First of all, let&rsquo;s try to import tensorflow and keras (which is now also part of tensorflow)"/>

  </head>

  <body class="ma0 avenir bg-near-white">

    
   
  

  <header>
    <div class="bg-black">
      <nav class="pv3 ph3 ph4-ns" role="navigation">
  <div class="flex-l justify-between items-center center">
    <a href="http://gofrendiasgard.github.io/" class="f3 fw2 hover-white no-underline white-90 dib">
      GoFrendi&#39;s Articles
    </a>
    <div class="flex-l items-center">
      

      
        <ul class="pl0 mr3">
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="/about/" title="Abouts page">
              Abouts
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="/posts/" title="Posts page">
              Posts
            </a>
          </li>
          
        </ul>
      
      


<a href="https://facebook.com/gofrendi" target="_blank" class="link-transition facebook link dib z-999 pt3 pt0-l mr1" title="Facebook link" rel="noopener" aria-label="follow on Facebook——Opens in a new window">
  <svg height="32px"  style="enable-background:new 0 0 67 67;" version="1.1" viewBox="0 0 67 67" width="32px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M28.765,50.32h6.744V33.998h4.499l0.596-5.624h-5.095  l0.007-2.816c0-1.466,0.14-2.253,2.244-2.253h2.812V17.68h-4.5c-5.405,0-7.307,2.729-7.307,7.317v3.377h-3.369v5.625h3.369V50.32z   M33,64C16.432,64,3,50.569,3,34S16.432,4,33,4s30,13.431,30,30S49.568,64,33,64z" style="fill-rule:evenodd;clip-rule:evenodd;"/></svg>

<span class="new-window"><svg  height="8px"  style="enable-background:new 0 0 1000 1000;" version="1.1" viewBox="0 0 1000 1000" width="8px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" >
<path d="M598 128h298v298h-86v-152l-418 418-60-60 418-418h-152v-86zM810 810v-298h86v298c0 46-40 86-86 86h-596c-48 0-86-40-86-86v-596c0-46 38-86 86-86h298v86h-298v596h596z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/>
</svg>
</span></a>


<a href="https://twitter.com/goFrendiAsgard" target="_blank" class="link-transition twitter link dib z-999 pt3 pt0-l mr1" title="Twitter link" rel="noopener" aria-label="follow on Twitter——Opens in a new window">
  <svg height="32px"  style="enable-background:new 0 0 67 67;" version="1.1" viewBox="0 0 67 67" width="32px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M37.167,22.283c-2.619,0.953-4.274,3.411-4.086,6.101  l0.063,1.038l-1.048-0.127c-3.813-0.487-7.145-2.139-9.974-4.915l-1.383-1.377l-0.356,1.017c-0.754,2.267-0.272,4.661,1.299,6.271  c0.838,0.89,0.649,1.017-0.796,0.487c-0.503-0.169-0.943-0.296-0.985-0.233c-0.146,0.149,0.356,2.076,0.754,2.839  c0.545,1.06,1.655,2.097,2.871,2.712l1.027,0.487l-1.215,0.021c-1.173,0-1.215,0.021-1.089,0.467  c0.419,1.377,2.074,2.839,3.918,3.475l1.299,0.444l-1.131,0.678c-1.676,0.976-3.646,1.526-5.616,1.568  C19.775,43.256,19,43.341,19,43.405c0,0.211,2.557,1.397,4.044,1.864c4.463,1.377,9.765,0.783,13.746-1.568  c2.829-1.673,5.657-5,6.978-8.221c0.713-1.716,1.425-4.851,1.425-6.354c0-0.975,0.063-1.102,1.236-2.267  c0.692-0.678,1.341-1.419,1.467-1.631c0.21-0.403,0.188-0.403-0.88-0.043c-1.781,0.636-2.033,0.551-1.152-0.402  c0.649-0.678,1.425-1.907,1.425-2.267c0-0.063-0.314,0.042-0.671,0.233c-0.377,0.212-1.215,0.53-1.844,0.72l-1.131,0.361l-1.027-0.7  c-0.566-0.381-1.361-0.805-1.781-0.932C39.766,21.902,38.131,21.944,37.167,22.283z M33,64C16.432,64,3,50.569,3,34S16.432,4,33,4  s30,13.431,30,30S49.568,64,33,64z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/></svg>

<span class="new-window"><svg  height="8px"  style="enable-background:new 0 0 1000 1000;" version="1.1" viewBox="0 0 1000 1000" width="8px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" >
<path d="M598 128h298v298h-86v-152l-418 418-60-60 418-418h-152v-86zM810 810v-298h86v298c0 46-40 86-86 86h-596c-48 0-86-40-86-86v-596c0-46 38-86 86-86h298v86h-298v596h596z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/>
</svg>
</span></a>









    </div>
  </div>
</nav>

    </div>
  </header>



    <main class="pb7" role="main">
      
  
  <article class="flex-l flex-wrap justify-between mw8 center ph3">

    <header class="mt4 w-100">
      <p class="f6 b helvetica tracked">
          
        POSTS
      </p>
      <h1 class="f1 athelas mb1">Neural Network From Scratch</h1>
      
      <time class="f6 mv4 dib tracked" datetime="2019-07-31T07:48:41&#43;07:00">July 31, 2019</time>
      
      
    </header>

    <section class="nested-copy-line-height lh-copy serif f4 nested-links nested-img mid-gray pr4-l w-two-thirds-l">

<h1 id="implementation-with-tensorflow">Implementation with Tensorflow</h1>

<p>TensorFlow is an open-source software library for dataflow programming across a range of tasks. It is a symbolic math library, and is also used for machine learning applications such as neural networks.</p>

<p>Unlike <code>sklearn.neural_network</code>, tensorflow give us more freedom to set up our neural-network.</p>

<p>The following implementation was taken directly from <a href="https://www.tensorflow.org/tutorials/keras/basic_classification">https://www.tensorflow.org/tutorials/keras/basic_classification</a></p>

<p>First of all, let&rsquo;s try to import tensorflow and keras (which is now also part of tensorflow)</p>

<h2 id="importing-tensorflow-and-keras">Importing Tensorflow and Keras</h2>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#6272a4"># TensorFlow and tf.keras</span>
<span style="color:#ff79c6">import</span> tensorflow <span style="color:#ff79c6">as</span> tf
<span style="color:#ff79c6">from</span> tensorflow <span style="color:#ff79c6">import</span> keras

<span style="color:#6272a4"># Helper libraries</span>
<span style="color:#ff79c6">import</span> numpy <span style="color:#ff79c6">as</span> np
<span style="color:#ff79c6">import</span> matplotlib.pyplot <span style="color:#ff79c6">as</span> plt</code></pre></div>
<h2 id="explore-the-dataset">Explore the dataset</h2>

<p>We will try to perform classification task on mnist&rsquo;s dataset (<a href="http://yann.lecun.com/exdb/mnist/">http://yann.lecun.com/exdb/mnist/</a>). The dataset contains of <code>70000</code> gray-scale images. Each image has <code>28 x 28</code> dimension and belong to one (and only one) of the following 10 classes:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">class_names <span style="color:#ff79c6">=</span> [<span style="color:#f1fa8c">&#39;zero&#39;</span>, <span style="color:#f1fa8c">&#39;one&#39;</span>, <span style="color:#f1fa8c">&#39;two&#39;</span>, <span style="color:#f1fa8c">&#39;three&#39;</span>, <span style="color:#f1fa8c">&#39;four&#39;</span>, <span style="color:#f1fa8c">&#39;five&#39;</span>, <span style="color:#f1fa8c">&#39;six&#39;</span>, <span style="color:#f1fa8c">&#39;seven&#39;</span>, <span style="color:#f1fa8c">&#39;eight&#39;</span>, <span style="color:#f1fa8c">&#39;nine&#39;</span>]</code></pre></div>
<p>Now, let&rsquo;s download <code>fashion_mnist</code> dataset from <code>keras.datasets</code> and split them into <code>train</code> and <code>test</code> set. By default, the dataset contains of <code>60000</code> training set and <code>10000</code> test set.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">mnist <span style="color:#ff79c6">=</span> keras<span style="color:#ff79c6">.</span>datasets<span style="color:#ff79c6">.</span>mnist

(train_images, train_labels), (test_images, test_labels) <span style="color:#ff79c6">=</span> mnist<span style="color:#ff79c6">.</span>load_data()</code></pre></div>
<p>Let&rsquo;s explore the data a little bit</p>

<p>Here is a bit information about our <code>train_labels</code>. It is a one-dimension array with 60000 elements</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">train_labels</code></pre></div>
<pre><code>array([5, 0, 4, ..., 5, 6, 8], dtype=uint8)
</code></pre>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">train_labels<span style="color:#ff79c6">.</span>shape</code></pre></div>
<pre><code>(60000,)
</code></pre>

<p>Now, let&rsquo;s explore our <code>train_images</code></p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">train_images</code></pre></div>
<pre><code>array([[[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        ...,
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0]],

       [[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        ...,
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0]],

       [[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        ...,
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0]],

       ...,

       [[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        ...,
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0]],

       [[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        ...,
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0]],

       [[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        ...,
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0]]], dtype=uint8)
</code></pre>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">train_images<span style="color:#ff79c6">.</span>shape</code></pre></div>
<pre><code>(60000, 28, 28)
</code></pre>

<p>Just to make sure, let&rsquo;s see our first image and label in detail</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">index <span style="color:#ff79c6">=</span> <span style="color:#bd93f9">0</span>
label <span style="color:#ff79c6">=</span> train_labels[index]
image <span style="color:#ff79c6">=</span> train_images[index]

<span style="color:#ff79c6">print</span>(<span style="color:#f1fa8c">&#34;label: &#34;</span>, label) <span style="color:#6272a4"># this is the first train_labels</span>
<span style="color:#ff79c6">print</span>(<span style="color:#f1fa8c">&#34;which is: &#34;</span>, class_names[label]) <span style="color:#6272a4"># use our pre-defined class_names to get textual representation of the label</span>
plt<span style="color:#ff79c6">.</span>figure()
plt<span style="color:#ff79c6">.</span>imshow(image) <span style="color:#6272a4"># if you just want to see the matrix representation of the image, use `image` instead</span>
plt<span style="color:#ff79c6">.</span>show()</code></pre></div>
<pre><code>label:  5
which is:  five
</code></pre>

<p><img src="neuralnetwork_files/neuralnetwork_14_1.png" alt="png" /></p>

<h2 id="configuring-the-neural-network-model">Configuring the neural network model</h2>

<p>Finally, let&rsquo;s build our neural network.</p>

<p>First of all, we define 3 layers here:</p>

<ul>
<li>flatten layer with input_shape = 28x28: This one will transform our 2 dimensional matrix into 1 dimensional matrix (or a vector). The output of this layer will be an array with 784 elements</li>
<li>dense layer containing 128 neuron with relu activation: This one will create a layer containing 128 neuron. Each of them is connected to the output of our previous layer (an array containing 784 elements). Each neuron activation is depending on <code>relu</code> function (<a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks">https://en.wikipedia.org/wiki/Rectifier_(neural_networks</a>)</li>
<li>dense layer containing 10 neuron with softmax activation: Finally, since we have 10 classes, it is natural to have 10 neuron in our output layer. Each neuron should show us how probable is an image belong to a particular class. Finally, we will use softmax to return the prediction result (<a href="https://en.wikipedia.org/wiki/Softmax_function">https://en.wikipedia.org/wiki/Softmax_function</a>)</li>
</ul>

<p>After defining the layers, we need to define our optimizer, loss function, and metrics:</p>

<ul>
<li>optimizer: How to optimize</li>
<li>loss function: How to calculate error</li>
<li>metrics: How to measure the quality of the network</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">model <span style="color:#ff79c6">=</span> keras<span style="color:#ff79c6">.</span>Sequential([
    keras<span style="color:#ff79c6">.</span>layers<span style="color:#ff79c6">.</span>Flatten(input_shape<span style="color:#ff79c6">=</span>(<span style="color:#bd93f9">28</span>, <span style="color:#bd93f9">28</span>)),
    keras<span style="color:#ff79c6">.</span>layers<span style="color:#ff79c6">.</span>Dense(<span style="color:#bd93f9">128</span>, activation<span style="color:#ff79c6">=</span>tf<span style="color:#ff79c6">.</span>nn<span style="color:#ff79c6">.</span>relu),
    keras<span style="color:#ff79c6">.</span>layers<span style="color:#ff79c6">.</span>Dense(<span style="color:#bd93f9">10</span>, activation<span style="color:#ff79c6">=</span>tf<span style="color:#ff79c6">.</span>nn<span style="color:#ff79c6">.</span>softmax)
])
model<span style="color:#ff79c6">.</span><span style="color:#8be9fd;font-style:italic">compile</span>(optimizer<span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#39;adam&#39;</span>,
              loss<span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#39;sparse_categorical_crossentropy&#39;</span>,
              metrics<span style="color:#ff79c6">=</span>[<span style="color:#f1fa8c">&#39;accuracy&#39;</span>])</code></pre></div>
<h2 id="train-the-neural-network-model">Train the neural network model</h2>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">model<span style="color:#ff79c6">.</span>fit(train_images, train_labels, epochs<span style="color:#ff79c6">=</span><span style="color:#bd93f9">100</span>)
test_loss, test_acc <span style="color:#ff79c6">=</span> model<span style="color:#ff79c6">.</span>evaluate(test_images, test_labels)
<span style="color:#ff79c6">print</span>(<span style="color:#f1fa8c">&#39;Test accuracy:&#39;</span>, test_acc)</code></pre></div>
<pre><code>Train on 60000 samples
Epoch 1/100
60000/60000 [==============================] - 6s 94us/sample - loss: 9.4768 - accuracy: 0.4109
Epoch 2/100
60000/60000 [==============================] - 5s 90us/sample - loss: 8.0383 - accuracy: 0.5005
Epoch 3/100
60000/60000 [==============================] - 5s 90us/sample - loss: 7.2688 - accuracy: 0.5484
Epoch 4/100
60000/60000 [==============================] - 5s 90us/sample - loss: 7.1353 - accuracy: 0.5569
Epoch 5/100
60000/60000 [==============================] - 5s 90us/sample - loss: 6.9730 - accuracy: 0.5669
Epoch 6/100
60000/60000 [==============================] - 5s 91us/sample - loss: 6.9570 - accuracy: 0.5681
Epoch 7/100
60000/60000 [==============================] - 5s 90us/sample - loss: 6.6057 - accuracy: 0.5896
Epoch 8/100
60000/60000 [==============================] - 5s 91us/sample - loss: 6.1260 - accuracy: 0.6193
Epoch 9/100
60000/60000 [==============================] - 5s 90us/sample - loss: 5.8815 - accuracy: 0.6345
Epoch 10/100
60000/60000 [==============================] - 5s 92us/sample - loss: 5.6206 - accuracy: 0.6507
Epoch 11/100
60000/60000 [==============================] - 5s 92us/sample - loss: 4.6773 - accuracy: 0.7093
Epoch 12/100
60000/60000 [==============================] - 5s 91us/sample - loss: 4.5473 - accuracy: 0.7174
Epoch 13/100
60000/60000 [==============================] - 6s 92us/sample - loss: 4.5345 - accuracy: 0.7182
Epoch 14/100
60000/60000 [==============================] - 6s 92us/sample - loss: 4.4753 - accuracy: 0.7219
Epoch 15/100
60000/60000 [==============================] - 6s 93us/sample - loss: 4.5269 - accuracy: 0.7186
Epoch 16/100
60000/60000 [==============================] - 6s 93us/sample - loss: 4.4178 - accuracy: 0.7256
Epoch 17/100
60000/60000 [==============================] - 5s 91us/sample - loss: 4.4035 - accuracy: 0.7265
Epoch 18/100
60000/60000 [==============================] - 6s 93us/sample - loss: 4.3175 - accuracy: 0.7318
Epoch 19/100
60000/60000 [==============================] - 5s 92us/sample - loss: 4.3240 - accuracy: 0.7314
Epoch 20/100
60000/60000 [==============================] - 6s 93us/sample - loss: 4.2989 - accuracy: 0.7329
Epoch 21/100
60000/60000 [==============================] - 6s 92us/sample - loss: 4.1754 - accuracy: 0.7407
Epoch 22/100
60000/60000 [==============================] - 6s 92us/sample - loss: 4.3550 - accuracy: 0.7295
Epoch 23/100
60000/60000 [==============================] - 6s 92us/sample - loss: 4.2358 - accuracy: 0.7369
Epoch 24/100
60000/60000 [==============================] - 6s 93us/sample - loss: 4.2346 - accuracy: 0.7370
Epoch 25/100
60000/60000 [==============================] - 6s 92us/sample - loss: 4.1078 - accuracy: 0.7448
Epoch 26/100
60000/60000 [==============================] - 6s 93us/sample - loss: 4.1685 - accuracy: 0.7412
Epoch 27/100
60000/60000 [==============================] - 6s 93us/sample - loss: 4.2378 - accuracy: 0.7368
Epoch 28/100
60000/60000 [==============================] - 6s 93us/sample - loss: 4.2262 - accuracy: 0.7375
Epoch 29/100
60000/60000 [==============================] - 6s 93us/sample - loss: 4.1886 - accuracy: 0.7399
Epoch 30/100
60000/60000 [==============================] - 6s 93us/sample - loss: 4.1263 - accuracy: 0.7438
Epoch 31/100
60000/60000 [==============================] - 6s 94us/sample - loss: 4.2394 - accuracy: 0.7368
Epoch 32/100
60000/60000 [==============================] - 6s 94us/sample - loss: 4.0048 - accuracy: 0.7514
Epoch 33/100
60000/60000 [==============================] - 6s 93us/sample - loss: 4.1238 - accuracy: 0.7440
Epoch 34/100
60000/60000 [==============================] - 6s 94us/sample - loss: 4.1525 - accuracy: 0.7421
Epoch 35/100
60000/60000 [==============================] - 6s 93us/sample - loss: 4.1032 - accuracy: 0.7453
Epoch 36/100
60000/60000 [==============================] - 6s 94us/sample - loss: 4.1175 - accuracy: 0.7443
Epoch 37/100
60000/60000 [==============================] - 6s 94us/sample - loss: 4.3530 - accuracy: 0.7297
Epoch 38/100
60000/60000 [==============================] - 6s 93us/sample - loss: 4.1691 - accuracy: 0.7412
Epoch 39/100
60000/60000 [==============================] - 5s 86us/sample - loss: 4.0590 - accuracy: 0.7480
Epoch 40/100
60000/60000 [==============================] - 5s 90us/sample - loss: 4.0694 - accuracy: 0.7473
Epoch 41/100
60000/60000 [==============================] - 5s 87us/sample - loss: 4.0510 - accuracy: 0.7485
Epoch 42/100
60000/60000 [==============================] - 5s 87us/sample - loss: 4.0803 - accuracy: 0.7467
Epoch 43/100
60000/60000 [==============================] - 5s 87us/sample - loss: 4.2356 - accuracy: 0.7370
Epoch 44/100
60000/60000 [==============================] - 5s 88us/sample - loss: 4.1049 - accuracy: 0.7452
Epoch 45/100
60000/60000 [==============================] - 5s 87us/sample - loss: 4.0528 - accuracy: 0.7484
Epoch 46/100
60000/60000 [==============================] - 5s 87us/sample - loss: 4.0291 - accuracy: 0.7498
Epoch 47/100
60000/60000 [==============================] - 5s 88us/sample - loss: 3.9522 - accuracy: 0.7547
Epoch 48/100
60000/60000 [==============================] - 6s 96us/sample - loss: 3.9992 - accuracy: 0.7517
Epoch 49/100
60000/60000 [==============================] - 6s 93us/sample - loss: 4.0291 - accuracy: 0.7499
Epoch 50/100
60000/60000 [==============================] - 6s 96us/sample - loss: 3.9588 - accuracy: 0.7542
Epoch 51/100
60000/60000 [==============================] - 5s 91us/sample - loss: 3.9611 - accuracy: 0.7541
Epoch 52/100
60000/60000 [==============================] - 5s 91us/sample - loss: 4.0488 - accuracy: 0.7487
Epoch 53/100
60000/60000 [==============================] - 6s 94us/sample - loss: 3.9929 - accuracy: 0.7522
Epoch 54/100
60000/60000 [==============================] - 6s 93us/sample - loss: 3.9734 - accuracy: 0.7533
Epoch 55/100
60000/60000 [==============================] - 6s 92us/sample - loss: 4.0326 - accuracy: 0.7496
Epoch 56/100
60000/60000 [==============================] - 6s 95us/sample - loss: 4.0214 - accuracy: 0.7504
Epoch 57/100
60000/60000 [==============================] - 6s 94us/sample - loss: 4.0237 - accuracy: 0.7502
Epoch 58/100
60000/60000 [==============================] - 6s 95us/sample - loss: 4.0089 - accuracy: 0.7511
Epoch 59/100
60000/60000 [==============================] - 6s 93us/sample - loss: 3.8984 - accuracy: 0.7580
Epoch 60/100
60000/60000 [==============================] - 6s 93us/sample - loss: 3.9781 - accuracy: 0.7530
Epoch 61/100
60000/60000 [==============================] - 6s 94us/sample - loss: 3.9318 - accuracy: 0.7560
Epoch 62/100
60000/60000 [==============================] - 6s 94us/sample - loss: 4.1118 - accuracy: 0.7448
Epoch 63/100
60000/60000 [==============================] - 6s 94us/sample - loss: 4.0048 - accuracy: 0.7514
Epoch 64/100
60000/60000 [==============================] - 6s 94us/sample - loss: 4.0615 - accuracy: 0.7479
Epoch 65/100
60000/60000 [==============================] - 6s 94us/sample - loss: 3.9387 - accuracy: 0.7555
Epoch 66/100
60000/60000 [==============================] - 6s 93us/sample - loss: 3.9485 - accuracy: 0.7548
Epoch 67/100
60000/60000 [==============================] - 6s 94us/sample - loss: 3.9242 - accuracy: 0.7564
Epoch 68/100
60000/60000 [==============================] - 6s 92us/sample - loss: 3.8543 - accuracy: 0.7608
Epoch 69/100
60000/60000 [==============================] - 6s 93us/sample - loss: 3.9845 - accuracy: 0.7527
Epoch 70/100
60000/60000 [==============================] - 6s 93us/sample - loss: 4.0102 - accuracy: 0.7510
Epoch 71/100
60000/60000 [==============================] - 6s 93us/sample - loss: 4.0308 - accuracy: 0.7498
Epoch 72/100
60000/60000 [==============================] - 6s 93us/sample - loss: 3.8557 - accuracy: 0.7606
Epoch 73/100
60000/60000 [==============================] - 6s 93us/sample - loss: 3.8924 - accuracy: 0.7584
Epoch 74/100
60000/60000 [==============================] - 6s 93us/sample - loss: 4.0663 - accuracy: 0.7476
Epoch 75/100
60000/60000 [==============================] - 6s 93us/sample - loss: 3.8677 - accuracy: 0.7599
Epoch 76/100
60000/60000 [==============================] - 5s 90us/sample - loss: 4.0528 - accuracy: 0.7485
Epoch 77/100
60000/60000 [==============================] - 5s 90us/sample - loss: 3.9462 - accuracy: 0.7551
Epoch 78/100
60000/60000 [==============================] - 5s 91us/sample - loss: 3.8967 - accuracy: 0.7581
Epoch 79/100
60000/60000 [==============================] - 5s 90us/sample - loss: 4.0119 - accuracy: 0.7510
Epoch 80/100
60000/60000 [==============================] - 5s 91us/sample - loss: 3.8886 - accuracy: 0.7587
Epoch 81/100
60000/60000 [==============================] - 5s 91us/sample - loss: 3.8775 - accuracy: 0.7594
Epoch 82/100
60000/60000 [==============================] - 5s 86us/sample - loss: 3.7995 - accuracy: 0.7642
Epoch 83/100
60000/60000 [==============================] - 5s 87us/sample - loss: 3.7972 - accuracy: 0.7643
Epoch 84/100
60000/60000 [==============================] - 5s 86us/sample - loss: 3.9107 - accuracy: 0.7572
Epoch 85/100
60000/60000 [==============================] - 5s 87us/sample - loss: 3.8611 - accuracy: 0.7604
Epoch 86/100
60000/60000 [==============================] - 5s 86us/sample - loss: 3.9305 - accuracy: 0.7560
Epoch 87/100
60000/60000 [==============================] - 5s 86us/sample - loss: 3.8945 - accuracy: 0.7583
Epoch 88/100
60000/60000 [==============================] - 5s 87us/sample - loss: 3.8754 - accuracy: 0.7595
Epoch 89/100
60000/60000 [==============================] - 5s 87us/sample - loss: 3.8740 - accuracy: 0.7596
Epoch 90/100
60000/60000 [==============================] - 6s 93us/sample - loss: 3.9132 - accuracy: 0.7571
Epoch 91/100
60000/60000 [==============================] - 6s 93us/sample - loss: 3.8803 - accuracy: 0.7591
Epoch 92/100
60000/60000 [==============================] - 6s 92us/sample - loss: 3.9693 - accuracy: 0.7537
Epoch 93/100
60000/60000 [==============================] - 5s 91us/sample - loss: 4.0703 - accuracy: 0.7474
Epoch 94/100
60000/60000 [==============================] - 6s 94us/sample - loss: 3.8868 - accuracy: 0.7588
Epoch 95/100
60000/60000 [==============================] - 5s 92us/sample - loss: 3.8406 - accuracy: 0.7616
Epoch 96/100
60000/60000 [==============================] - 6s 92us/sample - loss: 3.7705 - accuracy: 0.7660
Epoch 97/100
60000/60000 [==============================] - 6s 92us/sample - loss: 3.8150 - accuracy: 0.7632
Epoch 98/100
60000/60000 [==============================] - 5s 92us/sample - loss: 3.8774 - accuracy: 0.7593
Epoch 99/100
60000/60000 [==============================] - 5s 91us/sample - loss: 3.9108 - accuracy: 0.7573
Epoch 100/100
60000/60000 [==============================] - 5s 91us/sample - loss: 3.9316 - accuracy: 0.7560
10000/10000 [==============================] - 1s 55us/sample - loss: 3.8109 - accuracy: 0.7635
Test accuracy: 0.7635
</code></pre>

<h2 id="prediction">Prediction</h2>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">predictions <span style="color:#ff79c6">=</span> model<span style="color:#ff79c6">.</span>predict(test_images)
np<span style="color:#ff79c6">.</span>argmax(predictions[<span style="color:#bd93f9">0</span>])

index <span style="color:#ff79c6">=</span> <span style="color:#bd93f9">0</span>
prediction_label <span style="color:#ff79c6">=</span> np<span style="color:#ff79c6">.</span>argmax(predictions[<span style="color:#bd93f9">0</span>])
target_label <span style="color:#ff79c6">=</span> test_labels[index]
image <span style="color:#ff79c6">=</span> test_images[index]

<span style="color:#ff79c6">print</span>(<span style="color:#f1fa8c">&#34;target label: &#34;</span>, target_label, class_names[target_label])
<span style="color:#ff79c6">print</span>(<span style="color:#f1fa8c">&#34;prediction label: &#34;</span>, prediction_label, class_names[prediction_label])
plt<span style="color:#ff79c6">.</span>figure()
plt<span style="color:#ff79c6">.</span>imshow(image) <span style="color:#6272a4"># if you just want to see the matrix representation of the image, use `image` instead</span>
plt<span style="color:#ff79c6">.</span>show()</code></pre></div>
<pre><code>target label:  7 seven
prediction label:  7 seven
</code></pre>

<p><img src="neuralnetwork_files/neuralnetwork_20_1.png" alt="png" /></p>
<ul class="pa0">
  
   <li class="list">
     <a href="/tags/macine-learning" class="link f5 grow no-underline br-pill ba ph3 pv2 mb2 dib black sans-serif">Macine Learning</a>
   </li>
  
</ul>
<div class="mt6">
      
      
      </div>
    </section>

    <aside class="w-30-l mt6-l">




</aside>

  </article>

    </main>
    <footer class="bg-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href="http://gofrendiasgard.github.io/" >
    &copy; 2019 GoFrendi&#39;s Articles
  </a>
    <div>


<a href="https://facebook.com/gofrendi" target="_blank" class="link-transition facebook link dib z-999 pt3 pt0-l mr1" title="Facebook link" rel="noopener" aria-label="follow on Facebook——Opens in a new window">
  <svg height="32px"  style="enable-background:new 0 0 67 67;" version="1.1" viewBox="0 0 67 67" width="32px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M28.765,50.32h6.744V33.998h4.499l0.596-5.624h-5.095  l0.007-2.816c0-1.466,0.14-2.253,2.244-2.253h2.812V17.68h-4.5c-5.405,0-7.307,2.729-7.307,7.317v3.377h-3.369v5.625h3.369V50.32z   M33,64C16.432,64,3,50.569,3,34S16.432,4,33,4s30,13.431,30,30S49.568,64,33,64z" style="fill-rule:evenodd;clip-rule:evenodd;"/></svg>

<span class="new-window"><svg  height="8px"  style="enable-background:new 0 0 1000 1000;" version="1.1" viewBox="0 0 1000 1000" width="8px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" >
<path d="M598 128h298v298h-86v-152l-418 418-60-60 418-418h-152v-86zM810 810v-298h86v298c0 46-40 86-86 86h-596c-48 0-86-40-86-86v-596c0-46 38-86 86-86h298v86h-298v596h596z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/>
</svg>
</span></a>


<a href="https://twitter.com/goFrendiAsgard" target="_blank" class="link-transition twitter link dib z-999 pt3 pt0-l mr1" title="Twitter link" rel="noopener" aria-label="follow on Twitter——Opens in a new window">
  <svg height="32px"  style="enable-background:new 0 0 67 67;" version="1.1" viewBox="0 0 67 67" width="32px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M37.167,22.283c-2.619,0.953-4.274,3.411-4.086,6.101  l0.063,1.038l-1.048-0.127c-3.813-0.487-7.145-2.139-9.974-4.915l-1.383-1.377l-0.356,1.017c-0.754,2.267-0.272,4.661,1.299,6.271  c0.838,0.89,0.649,1.017-0.796,0.487c-0.503-0.169-0.943-0.296-0.985-0.233c-0.146,0.149,0.356,2.076,0.754,2.839  c0.545,1.06,1.655,2.097,2.871,2.712l1.027,0.487l-1.215,0.021c-1.173,0-1.215,0.021-1.089,0.467  c0.419,1.377,2.074,2.839,3.918,3.475l1.299,0.444l-1.131,0.678c-1.676,0.976-3.646,1.526-5.616,1.568  C19.775,43.256,19,43.341,19,43.405c0,0.211,2.557,1.397,4.044,1.864c4.463,1.377,9.765,0.783,13.746-1.568  c2.829-1.673,5.657-5,6.978-8.221c0.713-1.716,1.425-4.851,1.425-6.354c0-0.975,0.063-1.102,1.236-2.267  c0.692-0.678,1.341-1.419,1.467-1.631c0.21-0.403,0.188-0.403-0.88-0.043c-1.781,0.636-2.033,0.551-1.152-0.402  c0.649-0.678,1.425-1.907,1.425-2.267c0-0.063-0.314,0.042-0.671,0.233c-0.377,0.212-1.215,0.53-1.844,0.72l-1.131,0.361l-1.027-0.7  c-0.566-0.381-1.361-0.805-1.781-0.932C39.766,21.902,38.131,21.944,37.167,22.283z M33,64C16.432,64,3,50.569,3,34S16.432,4,33,4  s30,13.431,30,30S49.568,64,33,64z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/></svg>

<span class="new-window"><svg  height="8px"  style="enable-background:new 0 0 1000 1000;" version="1.1" viewBox="0 0 1000 1000" width="8px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" >
<path d="M598 128h298v298h-86v-152l-418 418-60-60 418-418h-152v-86zM810 810v-298h86v298c0 46-40 86-86 86h-596c-48 0-86-40-86-86v-596c0-46 38-86 86-86h298v86h-298v596h596z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/>
</svg>
</span></a>








</div>
  </div>
</footer>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<script type="text/javascript">
  
  const images = document.getElementsByTagName("img");
  for (const image of images) {
    const oldSrc = image.src;
    const correctedSrc = getCorrectedSrc(oldSrc);
    image.src = correctedSrc;
  }

  
  
  function getCorrectedSrc(oldSrc) {
    const regex = /(.*)\/([a-z0-9_\- ]*)\/([a-z0-9_\- ]*)_files\/(.*)/gmi;
    const matches = regex.exec(oldSrc);
    if (matches && matches[2] === matches[3]) {
      const prefix = matches[1];
      const title = matches[2];
      const sufix = matches[4];
      return [prefix, title+"_files", sufix].join("/");
    }
    return oldSrc;
  }
</script>

    

  <script src="/dist/js/app.3fc0f988d21662902933.js"></script>


  </body>
</html>
